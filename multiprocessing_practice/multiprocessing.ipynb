{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "8b05fc05ba4532d750856007996487d1091ef050e8a6d5b1a946cbc866ce0937"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-06-20 13:46:37 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Started\n",
      "2021-06-20 13:46:38 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/jack/Documents/Concurrency')\n",
    "from multiprocessing_practice.setup_logger import logger\n",
    "import time\n",
    "import numpy\n",
    "import onnxruntime as rt\n",
    "\n",
    "# load dataset\n",
    "# https://www.onnxruntime.ai/python/tutorial.html\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# load model\n",
    "filename = 'logreg_iris.onnx'\n",
    "\n",
    "def run():\n",
    "    # https://www.onnxruntime.ai/docs/how-to/tune-performance.html\n",
    "    sess_options = rt.SessionOptions()\n",
    "    sess_options.intra_op_num_threads = 1\n",
    "    sess = rt.InferenceSession(filename, sess_options=sess_options)\n",
    "    input_name = sess.get_inputs()[0].name\n",
    "\n",
    "    pred = sess.run(None, {input_name: X_test.astype(numpy.float32)})[0]\n",
    "\n",
    "# inference\n",
    "logger.debug('Started')\n",
    "begin = time.time()\n",
    "for _ in range(1000):\n",
    "    run()\n",
    "end = time.time()\n",
    "logger.debug('Finished')\n",
    "logger.info(f'Time Elapsed : {end-begin}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-06-20 15:00:32 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Started\n",
      "2021-06-20 15:00:33 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 0\n",
      "2021-06-20 15:00:34 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 1\n",
      "2021-06-20 15:00:34 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 2\n",
      "2021-06-20 15:00:35 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 3\n",
      "2021-06-20 15:00:36 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 4\n",
      "2021-06-20 15:00:37 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 5\n",
      "2021-06-20 15:00:38 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 6\n",
      "2021-06-20 15:00:39 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 7\n",
      "2021-06-20 15:00:39 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 8\n",
      "2021-06-20 15:00:40 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 9\n",
      "2021-06-20 15:00:41 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 10\n",
      "2021-06-20 15:00:42 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 11\n",
      "2021-06-20 15:00:43 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 12\n",
      "2021-06-20 15:00:44 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 13\n",
      "2021-06-20 15:00:45 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 14\n",
      "2021-06-20 15:00:45 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 15\n",
      "2021-06-20 15:00:46 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 16\n",
      "2021-06-20 15:00:47 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 17\n",
      "2021-06-20 15:00:48 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 18\n",
      "2021-06-20 15:00:49 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 19\n",
      "2021-06-20 15:00:49 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished\n",
      "2021-06-20 15:00:49 | MainThread |\u001b[32m INFO     \u001b[0m| root | Time Elapsed : 16.870883226394653\n",
      "2021-06-20 15:00:49 | MainThread |\u001b[32m INFO     \u001b[0m| root | Get 20 predictions\n"
     ]
    }
   ],
   "source": [
    "# file:///Users/jack/Downloads/PyTorch_Bert-Squad_OnnxRuntime_CPU.ipynb.html\n",
    "import sys\n",
    "sys.path.append('/Users/jack/Documents/Concurrency')\n",
    "from multiprocessing_practice.setup_logger import logger\n",
    "import time\n",
    "import numpy\n",
    "import torch\n",
    "import onnxruntime as rt\n",
    "\n",
    "max_seq_length = 128\n",
    "total_samples = 20\n",
    "\n",
    "dataset = torch.load('onnx_models/tensor_dataset.pt')\n",
    "\n",
    "filename = 'onnx_models/optimized_model_cpu.onnx'\n",
    "\n",
    "def run(i):\n",
    "    data = dataset[i]\n",
    "    rt_inputs = {\n",
    "        'input_ids':  data[0].cpu().reshape(1, max_seq_length).numpy(),\n",
    "        'input_mask': data[1].cpu().reshape(1, max_seq_length).numpy(),\n",
    "        'segment_ids': data[2].cpu().reshape(1, max_seq_length).numpy()\n",
    "    }\n",
    "\n",
    "    sess_options = rt.SessionOptions()\n",
    "    sess_options.intra_op_num_threads = 1\n",
    "    sess = rt.InferenceSession(filename, sess_options=sess_options)\n",
    "\n",
    "    pred = sess.run(None, rt_inputs)\n",
    "    logger.debug(f'Finished inference {i}')\n",
    "    return pred\n",
    "\n",
    "# inference\n",
    "preds = list()\n",
    "logger.debug('Started')\n",
    "begin = time.time()\n",
    "for i in range(total_samples):\n",
    "    pred = run(i)\n",
    "    preds.append(pred)\n",
    "end = time.time()\n",
    "logger.debug('Finished')\n",
    "logger.info(f'Time Elapsed : {end-begin}')\n",
    "logger.info(f'Get {len(preds)} predictions')"
   ]
  },
  {
   "source": [
    "Above example loads the model every time when executes run function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-06-20 14:57:10 | MainThread |\u001b[33m WARNING  \u001b[0m| root | context has already been set\n",
      "2021-06-20 14:57:10 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Started\n",
      "2021-06-20 14:57:16 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 7\n",
      "2021-06-20 14:57:16 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 3\n",
      "2021-06-20 14:57:16 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 2\n",
      "2021-06-20 14:57:16 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 1\n",
      "2021-06-20 14:57:16 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 0\n",
      "2021-06-20 14:57:16 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 6\n",
      "2021-06-20 14:57:16 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 4\n",
      "2021-06-20 14:57:16 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 5\n",
      "2021-06-20 14:57:18 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 8\n",
      "2021-06-20 14:57:18 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 9\n",
      "2021-06-20 14:57:18 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 11\n",
      "2021-06-20 14:57:18 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 10\n",
      "2021-06-20 14:57:18 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 12\n",
      "2021-06-20 14:57:18 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 13\n",
      "2021-06-20 14:57:18 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 14\n",
      "2021-06-20 14:57:18 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 15\n",
      "2021-06-20 14:57:19 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 16\n",
      "2021-06-20 14:57:19 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 17\n",
      "2021-06-20 14:57:19 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 18\n",
      "2021-06-20 14:57:19 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished inference 19\n",
      "2021-06-20 14:57:19 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished\n",
      "2021-06-20 14:57:19 | MainThread |\u001b[32m INFO     \u001b[0m| root | Time Elapsed : 8.858366012573242\n",
      "2021-06-20 14:57:19 | MainThread |\u001b[32m INFO     \u001b[0m| root | Get 20 predictions\n"
     ]
    }
   ],
   "source": [
    "# multiprocessing\n",
    "import sys\n",
    "sys.path.append('/Users/jack/Documents/Concurrency')\n",
    "from multiprocessing_practice.setup_logger import logger\n",
    "import time\n",
    "import numpy\n",
    "import torch\n",
    "import onnxruntime as rt\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "import psutil\n",
    "\n",
    "max_seq_length = 128\n",
    "total_samples = 20\n",
    "\n",
    "dataset = torch.load('onnx_models/tensor_dataset.pt')\n",
    "filename = 'onnx_models/optimized_model_cpu.onnx'\n",
    "\n",
    "def run(i):\n",
    "    data = dataset[i]\n",
    "    rt_inputs = {\n",
    "        'input_ids':  data[0].cpu().reshape(1, max_seq_length).numpy(),\n",
    "        'input_mask': data[1].cpu().reshape(1, max_seq_length).numpy(),\n",
    "        'segment_ids': data[2].cpu().reshape(1, max_seq_length).numpy()\n",
    "    }\n",
    "    \n",
    "    sess_options = rt.SessionOptions()\n",
    "    sess_options.intra_op_num_threads = 1\n",
    "    sess = rt.InferenceSession(filename, sess_options=sess_options)\n",
    "\n",
    "    pred = sess.run(None, rt_inputs)\n",
    "    logger.debug(f'Finished inference {i}')\n",
    "    return pred\n",
    "\n",
    "num_cpus = psutil.cpu_count(logical=False)\n",
    "try:\n",
    "    mp.set_start_method('fork')\n",
    "except RuntimeError as re:\n",
    "    logger.warning(re)\n",
    "pool = Pool(num_cpus)\n",
    "\n",
    "# inference\n",
    "logger.debug('Started')\n",
    "begin = time.time()\n",
    "preds = pool.map(run, range(total_samples))\n",
    "end = time.time()\n",
    "logger.debug('Finished')\n",
    "logger.info(f'Time Elapsed : {end-begin}')\n",
    "logger.info(f'Get {len(preds)} predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}