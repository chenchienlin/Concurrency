{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "8b05fc05ba4532d750856007996487d1091ef050e8a6d5b1a946cbc866ce0937"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-06-20 15:20:31 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Started\n",
      "2021-06-20 15:20:32 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished\n",
      "2021-06-20 15:20:32 | MainThread |\u001b[32m INFO     \u001b[0m| root | Time Elapsed : 1.1923861503601074\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/jack/Documents/Concurrency')\n",
    "from multiprocessing_practice.setup_logger import logger\n",
    "import time\n",
    "import numpy\n",
    "import onnxruntime as rt\n",
    "\n",
    "# load dataset\n",
    "# https://www.onnxruntime.ai/python/tutorial.html\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# load model\n",
    "filename = 'onnx_models/logreg_iris.onnx'\n",
    "\n",
    "def run():\n",
    "    # https://www.onnxruntime.ai/docs/how-to/tune-performance.html\n",
    "    sess_options = rt.SessionOptions()\n",
    "    sess_options.intra_op_num_threads = 1\n",
    "    sess = rt.InferenceSession(filename, sess_options=sess_options)\n",
    "    input_name = sess.get_inputs()[0].name\n",
    "\n",
    "    pred = sess.run(None, {input_name: X_test.astype(numpy.float32)})[0]\n",
    "\n",
    "# inference\n",
    "logger.debug('Started')\n",
    "begin = time.time()\n",
    "for _ in range(1000):\n",
    "    run()\n",
    "end = time.time()\n",
    "logger.debug('Finished')\n",
    "logger.info(f'Time Elapsed : {end-begin}')"
   ]
  },
  {
   "source": [
    "Above is the example for running the simplest model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-06-20 15:20:32 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Started\n",
      "2021-06-20 15:21:56 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished\n",
      "2021-06-20 15:21:56 | MainThread |\u001b[32m INFO     \u001b[0m| root | Time Elapsed : 83.88589191436768\n",
      "2021-06-20 15:21:56 | MainThread |\u001b[32m INFO     \u001b[0m| root | Get 100 predictions\n"
     ]
    }
   ],
   "source": [
    "# file:///Users/jack/Downloads/PyTorch_Bert-Squad_OnnxRuntime_CPU.ipynb.html\n",
    "import sys\n",
    "sys.path.append('/Users/jack/Documents/Concurrency')\n",
    "from multiprocessing_practice.setup_logger import logger\n",
    "import time\n",
    "import numpy\n",
    "import torch\n",
    "import onnxruntime as rt\n",
    "\n",
    "max_seq_length = 128\n",
    "total_samples = 100\n",
    "\n",
    "dataset = torch.load('onnx_models/tensor_dataset.pt')\n",
    "\n",
    "filename = 'onnx_models/optimized_model_cpu.onnx'\n",
    "\n",
    "def run(i):\n",
    "    data = dataset[i]\n",
    "    rt_inputs = {\n",
    "        'input_ids':  data[0].cpu().reshape(1, max_seq_length).numpy(),\n",
    "        'input_mask': data[1].cpu().reshape(1, max_seq_length).numpy(),\n",
    "        'segment_ids': data[2].cpu().reshape(1, max_seq_length).numpy()\n",
    "    }\n",
    "\n",
    "    sess_options = rt.SessionOptions()\n",
    "    sess_options.intra_op_num_threads = 1\n",
    "    sess = rt.InferenceSession(filename, sess_options=sess_options)\n",
    "\n",
    "    pred = sess.run(None, rt_inputs)\n",
    "    # logger.debug(f'Finished inference {i}')\n",
    "    return pred\n",
    "\n",
    "# inference\n",
    "preds = list()\n",
    "logger.debug('Started')\n",
    "begin = time.time()\n",
    "for i in range(total_samples):\n",
    "    pred = run(i)\n",
    "    preds.append(pred)\n",
    "end = time.time()\n",
    "logger.debug('Finished')\n",
    "logger.info(f'Time Elapsed : {end-begin}')\n",
    "logger.info(f'Get {len(preds)} predictions')"
   ]
  },
  {
   "source": [
    "Above, the model is loaded every time while running inference. It is probably the worst approach to run the model.  \n",
    "\n",
    "The reason for doing this is to compare with the next example because it is probably necessary to load models every time when inference task is delegated to another processor."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-06-20 15:21:56 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Started\n",
      "2021-06-20 15:22:30 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished\n",
      "2021-06-20 15:22:30 | MainThread |\u001b[32m INFO     \u001b[0m| root | Time Elapsed : 33.54128384590149\n",
      "2021-06-20 15:22:30 | MainThread |\u001b[32m INFO     \u001b[0m| root | Get 100 predictions\n"
     ]
    }
   ],
   "source": [
    "# multiprocessing\n",
    "import sys\n",
    "sys.path.append('/Users/jack/Documents/Concurrency')\n",
    "from multiprocessing_practice.setup_logger import logger\n",
    "import time\n",
    "import numpy\n",
    "import torch\n",
    "import onnxruntime as rt\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "import psutil\n",
    "\n",
    "max_seq_length = 128\n",
    "total_samples = 100\n",
    "\n",
    "dataset = torch.load('onnx_models/tensor_dataset.pt')\n",
    "filename = 'onnx_models/optimized_model_cpu.onnx'\n",
    "\n",
    "def run(i):\n",
    "    data = dataset[i]\n",
    "    rt_inputs = {\n",
    "        'input_ids':  data[0].cpu().reshape(1, max_seq_length).numpy(),\n",
    "        'input_mask': data[1].cpu().reshape(1, max_seq_length).numpy(),\n",
    "        'segment_ids': data[2].cpu().reshape(1, max_seq_length).numpy()\n",
    "    }\n",
    "    \n",
    "    sess_options = rt.SessionOptions()\n",
    "    sess_options.intra_op_num_threads = 1\n",
    "    sess = rt.InferenceSession(filename, sess_options=sess_options)\n",
    "\n",
    "    pred = sess.run(None, rt_inputs)\n",
    "    # logger.debug(f'Finished inference {i}')\n",
    "    return pred\n",
    "\n",
    "num_cpus = psutil.cpu_count(logical=False)\n",
    "try:\n",
    "    mp.set_start_method('fork')\n",
    "except RuntimeError as re:\n",
    "    logger.warning(re)\n",
    "\n",
    "# inference\n",
    "logger.debug('Started')\n",
    "begin = time.time()\n",
    "pool = Pool(num_cpus)\n",
    "preds = pool.map(run, range(total_samples))\n",
    "end = time.time()\n",
    "logger.debug('Finished')\n",
    "logger.info(f'Time Elapsed : {end-begin}')\n",
    "logger.info(f'Get {len(preds)} predictions')"
   ]
  },
  {
   "source": [
    "Based on above two examples, using multiprocessing does help without introducing another level of complexity."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-06-20 15:22:30 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Started\n",
      "2021-06-20 15:23:09 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished\n",
      "2021-06-20 15:23:09 | MainThread |\u001b[32m INFO     \u001b[0m| root | Time Elapsed : 39.2286479473114\n",
      "2021-06-20 15:23:09 | MainThread |\u001b[32m INFO     \u001b[0m| root | Get 100 predictions\n"
     ]
    }
   ],
   "source": [
    "# run preloaded model\n",
    "import sys\n",
    "sys.path.append('/Users/jack/Documents/Concurrency')\n",
    "from multiprocessing_practice.setup_logger import logger\n",
    "import time\n",
    "import numpy\n",
    "import torch\n",
    "import onnxruntime as rt\n",
    "\n",
    "max_seq_length = 128\n",
    "total_samples = 100\n",
    "\n",
    "dataset = torch.load('onnx_models/tensor_dataset.pt')\n",
    "\n",
    "filename = 'onnx_models/optimized_model_cpu.onnx'\n",
    "\n",
    "def run(sess, i):\n",
    "    data = dataset[i]\n",
    "    rt_inputs = {\n",
    "        'input_ids':  data[0].cpu().reshape(1, max_seq_length).numpy(),\n",
    "        'input_mask': data[1].cpu().reshape(1, max_seq_length).numpy(),\n",
    "        'segment_ids': data[2].cpu().reshape(1, max_seq_length).numpy()\n",
    "    }\n",
    "\n",
    "    pred = sess.run(None, rt_inputs)\n",
    "    # logger.debug(f'Finished inference {i}')\n",
    "    return pred\n",
    "\n",
    "# inference\n",
    "preds = list()\n",
    "logger.debug('Started')\n",
    "begin = time.time()\n",
    "\n",
    "sess_options = rt.SessionOptions()\n",
    "sess_options.intra_op_num_threads = 1\n",
    "sess = rt.InferenceSession(filename, sess_options=sess_options)\n",
    "\n",
    "for i in range(total_samples):\n",
    "    pred = run(sess, i)\n",
    "    preds.append(pred)\n",
    "\n",
    "end = time.time()\n",
    "logger.debug('Finished')\n",
    "logger.info(f'Time Elapsed : {end-begin}')\n",
    "logger.info(f'Get {len(preds)} predictions')"
   ]
  },
  {
   "source": [
    "The really reasonable scenario whould be loading the model once and it is not much slower than the previous multiprocessing example.  \n",
    "By these examples, it is obvious that loading models is a time comsuming task."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2021-06-20 15:23:09 | MainThread |\u001b[33m WARNING  \u001b[0m| root | context has already been set\n",
      "2021-06-20 15:23:09 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Started\n",
      "2021-06-20 15:23:11 | MainThread |\u001b[32m INFO     \u001b[0m| root | Init\n",
      "2021-06-20 15:23:11 | MainThread |\u001b[32m INFO     \u001b[0m| root | Init\n",
      "2021-06-20 15:23:11 | MainThread |\u001b[32m INFO     \u001b[0m| root | Init\n",
      "2021-06-20 15:23:11 | MainThread |\u001b[32m INFO     \u001b[0m| root | Init\n",
      "2021-06-20 15:23:11 | MainThread |\u001b[32m INFO     \u001b[0m| root | Init\n",
      "2021-06-20 15:23:11 | MainThread |\u001b[32m INFO     \u001b[0m| root | Init\n",
      "2021-06-20 15:23:11 | MainThread |\u001b[32m INFO     \u001b[0m| root | Init\n",
      "2021-06-20 15:23:11 | MainThread |\u001b[32m INFO     \u001b[0m| root | Init\n",
      "2021-06-20 15:23:18 | MainThread |\u001b[36m DEBUG    \u001b[0m| root | Finished\n",
      "2021-06-20 15:23:18 | MainThread |\u001b[32m INFO     \u001b[0m| root | Time Elapsed : 9.415037870407104\n",
      "2021-06-20 15:23:18 | MainThread |\u001b[32m INFO     \u001b[0m| root | Get 100 predictions\n"
     ]
    }
   ],
   "source": [
    "# multiprocessing with initializer\n",
    "import sys\n",
    "sys.path.append('/Users/jack/Documents/Concurrency')\n",
    "from multiprocessing_practice.setup_logger import logger\n",
    "import time\n",
    "import numpy\n",
    "import torch\n",
    "import onnxruntime as rt\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "import psutil\n",
    "\n",
    "max_seq_length = 128\n",
    "total_samples = 100\n",
    "\n",
    "dataset = torch.load('onnx_models/tensor_dataset.pt')\n",
    "filename = 'onnx_models/optimized_model_cpu.onnx'\n",
    "\n",
    "def run(i):\n",
    "    global sess\n",
    "    data = dataset[i]\n",
    "    rt_inputs = {\n",
    "        'input_ids':  data[0].cpu().reshape(1, max_seq_length).numpy(),\n",
    "        'input_mask': data[1].cpu().reshape(1, max_seq_length).numpy(),\n",
    "        'segment_ids': data[2].cpu().reshape(1, max_seq_length).numpy()\n",
    "    }\n",
    "    pred = sess.run(None, rt_inputs)\n",
    "    # logger.debug(f'Finished inference {i}')\n",
    "    return pred\n",
    "\n",
    "sess = None\n",
    "def init():\n",
    "    global sess\n",
    "    sess_options = rt.SessionOptions()\n",
    "    sess_options.intra_op_num_threads = 1\n",
    "    sess = rt.InferenceSession(filename, sess_options=sess_options)\n",
    "    logger.info('Init')\n",
    "\n",
    "\n",
    "num_cpus = psutil.cpu_count(logical=False)\n",
    "try:\n",
    "    mp.set_start_method('fork')\n",
    "except RuntimeError as re:\n",
    "    logger.warning(re)\n",
    "\n",
    "# inference\n",
    "logger.debug('Started')\n",
    "begin = time.time()\n",
    "pool = Pool(num_cpus, initializer=init)\n",
    "preds = pool.map(run, range(total_samples))\n",
    "end = time.time()\n",
    "logger.debug('Finished')\n",
    "logger.info(f'Time Elapsed : {end-begin}')\n",
    "logger.info(f'Get {len(preds)} predictions')"
   ]
  },
  {
   "source": [
    "Because multiprocessing library provides initializer function, the model is only loaded at the begining."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}